{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d9c8bab2-b68b-4094-91db-6e26aceda2e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook.services'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnotebook\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigManager\n\u001b[1;32m      2\u001b[0m cm \u001b[38;5;241m=\u001b[39m ConfigManager()\n\u001b[1;32m      3\u001b[0m cm\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotebook\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook.services'"
     ]
    }
   ],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.get('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "57933ad0-4e8d-42ce-bc4f-2c8a205c6a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "8900671232\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.mps.current_allocated_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "825e255a-fe21-4a6c-a1ec-1ae8522639f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated VRAM needed: 17.4GB\n",
      "✅ Model fits in M3 memory\n"
     ]
    }
   ],
   "source": [
    "# Memory validation tool for Apple Silicon\n",
    "def check_model_fit(model):\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    mem_required = (param_count * 2) / 1e9 * 1.2  # FP16 calculation\n",
    "    print(f\"Estimated VRAM needed: {mem_required:.1f}GB\")\n",
    "    return mem_required < 20  # 24GB - 4GB system reserve\n",
    "\n",
    "if check_model_fit(model):\n",
    "    print(\"✅ Model fits in M3 memory\")\n",
    "else:\n",
    "    print(\"❌ Reduce model size or use quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698cbd06-de0e-4484-99e4-e12e037ec0ca",
   "metadata": {},
   "source": [
    "**Machine information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6e0f049-cd41-4e6b-87c5-b167a59d02c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Info: {'device': 'MPS', 'total_ram_gb': 24.0, 'gpu_available': True, 'gpu_name': 'Apple Silicon M3'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "def get_machine_info():\n",
    "    \"\"\"Fetch machine hardware information.\"\"\"\n",
    "    info = {\n",
    "        \"device\": \"MPS\" if torch.backends.mps.is_available() else \"CPU\",\n",
    "        \"total_ram_gb\": round(psutil.virtual_memory().total / (1024 ** 3), 2),\n",
    "        \"gpu_available\": torch.backends.mps.is_available(),\n",
    "        \"gpu_name\": \"Apple Silicon M3\" if torch.backends.mps.is_available() else None,\n",
    "    }\n",
    "    return info\n",
    "\n",
    "# Example usage\n",
    "machine_info = get_machine_info()\n",
    "print(\"Machine Info:\", machine_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6fdcfb-2b66-47bc-ab80-13a8d2dccb74",
   "metadata": {},
   "source": [
    "**Training estimate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "09ec7222-3179-4cd0-bf68-b390426c5f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Training RAM: 52.16 GB\n"
     ]
    }
   ],
   "source": [
    "def estimate_training_ram(model, batch_size, seq_length=2048, precision=\"fp16\"):\n",
    "    \"\"\"\n",
    "    Estimate RAM needed for training.\n",
    "    \n",
    "    Args:\n",
    "        model: The model object (e.g., Hugging Face transformer).\n",
    "        batch_size: Number of samples per batch.\n",
    "        seq_length: Maximum sequence length (tokens).\n",
    "        precision: Precision type (\"fp32\", \"fp16\", or \"int8\").\n",
    "    \n",
    "    Returns:\n",
    "        Estimated RAM in GB.\n",
    "    \"\"\"\n",
    "    # Number of parameters in the model\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Bytes per parameter based on precision\n",
    "    precision_bytes = {\"fp32\": 4, \"fp16\": 2, \"int8\": 1}.get(precision.lower(), 2)\n",
    "    \n",
    "    # Model memory (parameters + optimizer states)\n",
    "    model_memory_gb = (param_count * precision_bytes * 3) / 1e9  # Model + Gradients + Optimizer\n",
    "    \n",
    "    # Activation memory (batch size x sequence length x hidden size x bytes)\n",
    "    hidden_size = model.config.hidden_size\n",
    "    activation_memory_gb = (batch_size * seq_length * hidden_size * precision_bytes) / 1e9\n",
    "    \n",
    "    # Total memory with overhead\n",
    "    total_memory_gb = (model_memory_gb + activation_memory_gb) * 1.2  # Add ~20% overhead\n",
    "    \n",
    "    return round(total_memory_gb, 2)\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_length = 1024\n",
    "training_ram = estimate_training_ram(model, batch_size, seq_length)\n",
    "print(f\"Estimated Training RAM: {training_ram} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "59dfa64e-8f9a-4f5f-94bf-53542135ee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Inference RAM: 17.39 GB\n"
     ]
    }
   ],
   "source": [
    "def estimate_inference_ram(model, seq_length=2048, precision=\"fp16\"):\n",
    "    \"\"\"\n",
    "    Estimate RAM needed for inference.\n",
    "    \n",
    "    Args:\n",
    "        model: The model object (e.g., Hugging Face transformer).\n",
    "        seq_length: Maximum sequence length (tokens).\n",
    "        precision: Precision type (\"fp32\", \"fp16\", or \"int8\").\n",
    "    \n",
    "    Returns:\n",
    "        Estimated RAM in GB.\n",
    "    \"\"\"\n",
    "    # Number of parameters in the model\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Bytes per parameter based on precision\n",
    "    precision_bytes = {\"fp32\": 4, \"fp16\": 2, \"int8\": 1}.get(precision.lower(), 2)\n",
    "    \n",
    "    # Model memory only (parameters)\n",
    "    model_memory_gb = (param_count * precision_bytes) / 1e9\n",
    "    \n",
    "    # Activation memory (sequence length x hidden size x bytes)\n",
    "    hidden_size = model.config.hidden_size\n",
    "    activation_memory_gb = (seq_length * hidden_size * precision_bytes) / 1e9\n",
    "    \n",
    "    # Total memory with overhead\n",
    "    total_memory_gb = (model_memory_gb + activation_memory_gb) * 1.2  # Add ~20% overhead\n",
    "    \n",
    "    return round(total_memory_gb, 2)\n",
    "\n",
    "# Example usage\n",
    "seq_length = 1024\n",
    "inference_ram = estimate_inference_ram(model, seq_length)\n",
    "print(f\"Estimated Inference RAM: {inference_ram} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e0b12ee-979a-4d84-9d44-7bbbac988eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c652d724-37e4-4c25-a6d0-7bb7dd213622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "MODEL_NAMES = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"phi2\": \"microsoft/phi-2\",\n",
    "    \"qwen\": \"Qwen/Qwen2.5-0.5B\"\n",
    "}\n",
    "\n",
    "# Class Labels\n",
    "CLASS_LABELS = [\n",
    "    \"0_not_relevant\", \"1_not_happening\", \"2_not_human\", \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\", \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\", \"7_fossil_fuels_needed\"\n",
    "]\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Suppress multiprocessing warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "218c0c98-67a0-41bf-b384-88953de363e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading Function\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94ff402b-3f51-4faf-b304-8f01aefb138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "def create_prompt(row):\n",
    "    statement = row['quote']\n",
    "    category = row['label'].split('_')[0]\n",
    "    instruction = f\"\"\"[INST] <<SYS>>\n",
    "Classify the following statement into one of these 8 categories:\n",
    "Respond STRICTLY with only the corresponding number.\n",
    "<</SYS>>\n",
    "\n",
    "<categories>\n",
    "0 - Not relevant: No climate-related claims or doesn't fit other categories\n",
    "1 - Denial: Claims climate change is not happening\n",
    "2 - Attribution denial: Claims human activity is not causing climate change\n",
    "3 - Impact minimization: Claims climate change impacts are minimal or beneficial\n",
    "4 - Solution opposition: Claims solutions to climate change are harmful\n",
    "5 - Science skepticism: Challenges climate science validity or methods\n",
    "6 - Actor criticism: Attacks credibility of climate scientists or activists\n",
    "7 - Fossil fuel promotion: Asserts importance of fossil fuels\n",
    "</categories>\n",
    "<statement>{statement}</statement>\n",
    "[/INST]\n",
    "Category : \"\"\"\n",
    "    return {\"text\": \"<s>\" + instruction + \"<category>\" + category + \"<category></s>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "919f9654-80f3-4ba2-ae87-ee65635fb58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_parquet(\"hf://datasets/QuotaClimat/frugalaichallenge-text-train/train.parquet\")\n",
    "\n",
    "# Train-Test Split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Sample for Baseline Training\n",
    "N_SAMPLES = 50\n",
    "df_sampled_train = train_df.sample(N_SAMPLES, random_state=42)\n",
    "df_sampled_val = val_df.sample(N_SAMPLES, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8d1f2be-d7ad-4a84-bad1-f4c343b2962d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<s>[INST] <<SYS>>\\nClassify the following statement into one of these 8 categories:\\nRespond STRICTLY with only the corresponding number.\\n<</SYS>>\\n\\n<categories>\\n0 - Not relevant: No climate-related claims or doesn't fit other categories\\n1 - Denial: Claims climate change is not happening\\n2 - Attribution denial: Claims human activity is not causing climate change\\n3 - Impact minimization: Claims climate change impacts are minimal or beneficial\\n4 - Solution opposition: Claims solutions to climate change are harmful\\n5 - Science skepticism: Challenges climate science validity or methods\\n6 - Actor criticism: Attacks credibility of climate scientists or activists\\n7 - Fossil fuel promotion: Asserts importance of fossil fuels\\n</categories>\\n<statement>With the carbon tax to be terminated, attention therefore needs to be turned to repealing the renewable requirements.</statement>\\n[/INST]\\nCategory : <category>4<category></s>\"}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format Dataset\n",
    "train_data = df_sampled_train.apply(create_prompt, axis=1).tolist()\n",
    "val_data = df_sampled_val.apply(create_prompt, axis=1).tolist()\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "57f3f72f-060f-4609-a00d-a0af7e583483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e1357475a14b109e1bee32c8e232cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   456,   349,   272, 13126]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Model\n",
    "selected_model = \"mistral\"\n",
    "model_name = MODEL_NAMES[selected_model]\n",
    "\n",
    "tokenizer, model = load_model(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.truncation_side = 'right'\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "tokenizer(\"this is the instruction\", return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "562e4688-b2f9-4b37-b39a-f7749b67b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(train_data):\n",
    "    return tokenizer(train_data[\"text\"], padding=\"max_length\", truncation=True, max_length=2048, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a4bcc08-cd83-4eac-a6de-451f92a343d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7171ed7a77b423b9bc0f6ac09e7541c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa112e7aac741b9ab7feebb9e286c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert Data to HF Dataset Format\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data).map(tokenize_function, batched=True)\n",
    "val_dataset = Dataset.from_list(val_data).map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fe6ea529-b8ac-4cb2-b036-65bd71331be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=False,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=None,\n",
       "eval_strategy=no,\n",
       "eval_use_gather_object=False,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./models/runs/Feb03_16-58-47_AMAFHP9MXRXX1,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./models,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['codecarbon'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./models,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_liger_kernel=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Configuration\n",
    "use_fp16 = torch.cuda.is_available()  \n",
    "use_bf16 = torch.cuda.is_bf16_supported() and not torch.backends.mps.is_available()\n",
    "\"\"\"\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=use_fp16,\n",
    "    bf16=use_bf16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    "\"\"\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "47d2970a-e03b-43f8-969f-227b6dee219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Trainer Setup\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:307\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processing_class\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m processing_class\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlead to some unexpected behaviour due to overflow issues when training a model in half-precision. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou might consider adding `processing_class.padding_side = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to your code.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/transformers/trainer.py:607\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    606\u001b[0m ):\n\u001b[0;32m--> 607\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/transformers/trainer.py:888\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 888\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/accelerate/big_modeling.py:458\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "# Trainer Setup\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da2c66-1274-4342-8dff-be2391312b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1637b3e-9d53-487a-abbd-8105b264a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "trainer.save_model(\"./models/fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca782f32-5550-4734-ab59-9828cabcb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Evaluation\n",
    "def compute_accuracy(predictions, labels):\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    return np.mean(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a385d3-72b8-4694-8454-9da116f3dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(val_dataset).predictions\n",
    "labels = np.array(val_dataset[\"label\"])\n",
    "accuracy = compute_accuracy(predictions, labels)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e812cb1-d396-471d-b69d-7daa8352f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae61e12-620b-4da8-a7f6-757583406da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "MODEL_NAMES = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"phi2\": \"microsoft/phi-2\",\n",
    "    \"qwen\": \"Qwen/Qwen2.5-0.5B\"\n",
    "}\n",
    "\n",
    "# Class Labels\n",
    "CLASS_LABELS = [\n",
    "    \"0_not_relevant\", \"1_not_happening\", \"2_not_human\", \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\", \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\", \"7_fossil_fuels_needed\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de560cf3-e3dc-453d-8d80-68d2c3ec753d",
   "metadata": {},
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9237d42-79c8-495b-93e8-d52c66f071b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading Function\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "def load_model(model_name):\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=model_name,\n",
    "        task=\"text-generation\",\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "    )\n",
    "\n",
    "#    chat_model = ChatHuggingFace(llm=llm)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3295790c-4281-42b6-9e13-9eddf2b1c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.1', repetition_penalty=1.03, stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.1', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.1', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.1', timeout=120)>, task='text-generation')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Model\n",
    "selected_model = \"mistral\"\n",
    "model_name = MODEL_NAMES[selected_model]\n",
    "\n",
    "model = load_model(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d4d2a1f-2c17-41ba-afbf-ac03dfb1601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mmodels--Qwen--Qwen2.5-0.5B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--Qwen--Qwen2.5-7B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--mistralai--Mistral-7B-Instruct-v0.1\u001b[m\u001b[m\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b16f988b-6d63-4490-ab82-bb3ed9a5ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11M\t/Users/a.villa.massone/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B\n"
     ]
    }
   ],
   "source": [
    "!du -sh ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8be19259-38b4-49ce-80d5-fbad0ebfc237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.villa.massone/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Assistant: AI, or Artificial Intelligence, refers to the development of computer systems able to perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding. AI can be categorized into three main types: - Narrow or Weak AI: Designed to perform a single task (like this chatbot). - General or Strong AI: Understands and learns any intellectual task that a human can do. - Superintelligent AI: Hypothetical AI that possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every economically valuable work.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(content=\"What is AI?\")]\n",
    "response = model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18a5896b-633f-4e7a-8a84-af158e675654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm a new user of this chatbot. What can you do for me?\n",
      "Chatbot: Hello! I'm here to help with a wide range of tasks. Here are some examples:\n",
      "\n",
      "1. **Answer Questions**: I can provide information based on the data I've been trained on, up until 2021.\n",
      "2. **Explain Concepts**: I can help explain complex ideas in a simple way.\n",
      "3. **Engage in Dialogue**: I can participate in conversations on a variety of topics.\n",
      "4. **Provide Suggestions**: Whether it's a book to read, a movie to watch, or a recipe to cook, I can offer recommendations.\n",
      "5. **Help with Language**: I can help with language translations, define words, or provide synonyms and antonyms.\n",
      "\n",
      "What would you like to do today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke([HumanMessage(\"hi!\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4f886b2-f45a-45bc-a6d0-2699aaa09406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = model.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48d71129-e7d6-463b-8fe4-40028180a4ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEndpoint' object has no attribute 'withStructuredOutput'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m structuredLlm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithStructuredOutput\u001b[49m(joke, { name: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoke\u001b[39m\u001b[38;5;124m\"\u001b[39m });\n\u001b[1;32m      3\u001b[0m structuredLlm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a joke about cats\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n",
      "File \u001b[0;32m~/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/pydantic/main.py:891\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceEndpoint' object has no attribute 'withStructuredOutput'"
     ]
    }
   ],
   "source": [
    "structuredLlm = model.withStructuredOutput(joke, { name: \"joke\" });\n",
    "\n",
    "structuredLlm.invoke(\"Tell me a joke about cats\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "491bb4f6-0e25-4bd6-ad14-f37cefadd0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Can it think?\n",
      "\n",
      "AI, or Artificial Intelligence, refers to the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, problem-solving, perception, and language understanding. However, when you ask if AI can \"think,\" it's important to understand that AI doesn't think in the same way humans do.\n",
      "\n",
      "AI systems don't have consciousness, self-awareness, or personal experiences, which are often associated with human thinking. Instead, they process data and make decisions based on algorithms and statistical models. Here's a simple breakdown:\n",
      "\n",
      "1. **Learning**: AI can learn from data, improving its performance on a specific task over time. This can be done through various methods like supervised learning, unsupervised learning, or reinforcement learning.\n",
      "\n",
      "2. **Reasoning**: AI can reason using logical rules or statistical models to make decisions or predictions.\n",
      "\n",
      "3. **Problem-solving**: AI can find solutions for complex problems, often by searching through a large number of possible solutions.\n",
      "\n",
      "4. **Perception**: AI can interpret and understand data from the environment, such as images, speech, or text, using techniques like computer vision or natural language processing.\n",
      "\n",
      "5. **Language understanding**: AI can understand and generate human language, up to a point. It can respond to commands, questions, or statements, but it doesn't understand language in the same way humans do.\n",
      "\n",
      "So, while AI can perform tasks that might seem intelligent, it doesn't think or reason in the same way humans do. It's more about processing and analyzing data to make decisions or predictions based on that data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What is AI?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = model.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04f29b4d-b670-4851-b180-fc5829d07f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Louis XIX was a king of France for about 20 minutes on August 10, 1830. He was the son of King Charles X and succeeded his brother, Charles X, after a revolution that forced Charles X to abdicate. However, Louis XIX himself abdicated the throne shortly after ascending to it, passing the crown to his nephew, Henri, who became King Henri V. Louis XIX lived out the rest of his life in exile and died in 1844.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"Who is Louis XIX\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = model.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e02ecd-8b10-4e4f-8f62-d37b867f23e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

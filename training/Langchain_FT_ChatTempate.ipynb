{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552cd2dc-a040-4698-830f-0d78362f1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                     MAC SPECIFICATIONS                     \n",
      "============================================================\n",
      "\n",
      "                     System Information                     \n",
      "------------------------------------------------------------\n",
      "Model: Mac15,12\n",
      "OS: macOS 15.3.1\n",
      "Architecture: arm64 (Apple Silicon)\n",
      "\n",
      "                      CPU Information                       \n",
      "------------------------------------------------------------\n",
      "Processor: Apple M3\n",
      "Physical cores: 8\n",
      "Logical cores: 8\n",
      "\n",
      "                     Memory Information                     \n",
      "------------------------------------------------------------\n",
      "Total RAM: 24.0 GB\n",
      "Available RAM: 9.97 GB\n",
      "\n",
      "                    Storage Information                     \n",
      "------------------------------------------------------------\n",
      "Total Disk: 460.43 GB\n",
      "Free Disk: 303.38 GB\n",
      "\n",
      "                     GPU/ML Information                     \n",
      "------------------------------------------------------------\n",
      "PyTorch Version: 2.6.0\n",
      "MPS Available: Yes\n",
      "\n",
      "============================================================\n",
      "                 FINE-TUNING RECOMMENDATION                 \n",
      "============================================================\n",
      "\n",
      "Your Mac may be able to fine-tune Phi-3-mini with significant optimizations.\n",
      "Recommendations:\n",
      "- Use QLoRA with 4-bit quantization\n",
      "- Set a very small batch size (1) with gradient accumulation\n",
      "- Enable gradient checkpointing\n",
      "- Consider reducing context length (e.g., 512 instead of 4096)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from frugalai.utils.hardware import print_mac_specs\n",
    "\n",
    "print_mac_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "590988c1-972f-4977-9cba-62ffb72a4f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                   SYSTEM SPECIFICATIONS                    \n",
      "============================================================\n",
      "\n",
      "                     System Information                     \n",
      "------------------------------------------------------------\n",
      "Hostname: AMAFHP9MXRXX1\n",
      "OS: Darwin 24.3.0 Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:23 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8122\n",
      "Model: Mac15,12\n",
      "Architecture: arm64 (Apple Silicon)\n",
      "Environment: Physical Machine\n",
      "\n",
      "                      CPU Information                       \n",
      "------------------------------------------------------------\n",
      "Processor: Apple M3\n",
      "Physical cores: 8\n",
      "Logical cores: 8\n",
      "Frequency: Unknown\n",
      "\n",
      "                     Memory Information                     \n",
      "------------------------------------------------------------\n",
      "Total RAM: 24.00 GB\n",
      "Available RAM: 10.01 GB\n",
      "Used RAM: 12.14 GB (58.3%)\n",
      "\n",
      "                    Storage Information                     \n",
      "------------------------------------------------------------\n",
      "Total Disk: 2.25 TB\n",
      "Free Disk: 1.48 TB\n",
      "\n",
      "Mounted Partitions:\n",
      "  / (apfs): 460.43 GB total, 303.39 GB free\n",
      "  /System/Volumes/VM (apfs): 460.43 GB total, 303.39 GB free\n",
      "  /System/Volumes/Preboot (apfs): 460.43 GB total, 303.39 GB free\n",
      "  /System/Volumes/Update (apfs): 460.43 GB total, 303.39 GB free\n",
      "  /System/Volumes/xarts (apfs): 0.49 GB total, 0.47 GB free\n",
      "  /System/Volumes/iSCPreboot (apfs): 0.49 GB total, 0.47 GB free\n",
      "  /System/Volumes/Hardware (apfs): 0.49 GB total, 0.47 GB free\n",
      "  /System/Volumes/Data (apfs): 460.43 GB total, 303.39 GB free\n",
      "  /Volumes/Obsidian 1.8.7-universal (hfs): 0.48 GB total, 0.05 GB free\n",
      "\n",
      "                     GPU/ML Information                     \n",
      "------------------------------------------------------------\n",
      "PyTorch Version: 2.6.0\n",
      "CUDA Available: No\n",
      "MPS Available: Yes (Apple Silicon acceleration)\n",
      "TensorFlow Version: 2.18.0\n",
      "\n",
      "GPU Information:\n",
      "  GPU 1: Apple M3\n",
      "\n",
      "============================================================\n",
      "                 ML TRAINING RECOMMENDATION                 \n",
      "============================================================\n",
      "\n",
      "Your Apple Silicon Mac may be able to train with significant optimizations.\n",
      "Recommendations:\n",
      "- Use QLoRA with 4-bit quantization\n",
      "- Set a very small batch size (1) with gradient accumulation\n",
      "- Enable gradient checkpointing\n",
      "- Consider reducing context length\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from frugalai.utils.hardware import print_system_specs\n",
    "\n",
    "print_system_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0d3b68-9daf-4c2d-9e5e-2ad61b5f7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ddba8-599c-408c-bf51-94dd5ae894e8",
   "metadata": {},
   "source": [
    "### Fine-tuning LLM text-classification with transformers chattemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9c33ec-f865-4394-b8d3-18ca5e96e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frugalai.utils.efficiency_tracker import FunctionTracker\n",
    "\n",
    "tracker = FunctionTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ce1ed-b610-4456-b581-e398ea18c5af",
   "metadata": {},
   "source": [
    "##### **Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c78963e-dde7-42d8-9152-ff65455130a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"0_not_relevant\": 0,\n",
    "    \"1_not_happening\": 1,\n",
    "    \"2_not_human\": 2,\n",
    "    \"3_not_bad\": 3,\n",
    "    \"4_solutions_harmful_unnecessary\": 4,\n",
    "    \"5_science_unreliable\": 5,\n",
    "    \"6_proponents_biased\": 6,\n",
    "    \"7_fossil_fuels_needed\": 7,\n",
    "}\n",
    "\n",
    "id2label = {int(v): k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a41a70e-ece6-4660-becc-61a79f500439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ FunctionTimer: load_frugalai_dataset\n",
      "| time            00:00:07.8952\n",
      "| emissions       0.000000 CO2eq\n",
      "| energy consumed 0.000005 kWh\n",
      "\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "{'train': (4872, 2), 'test': (1219, 2)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "@tracker.track\n",
    "def load_frugalai_dataset():\n",
    "    \n",
    "    ds = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\")\n",
    "    ds = ds.select_columns(['quote', 'label'])\n",
    "    ds = ds.map(lambda x: {\"label\": label2id[x[\"label\"]]}, batched=False)\n",
    "    return ds\n",
    "\n",
    "ds = load_frugalai_dataset()\n",
    "print(type(ds))\n",
    "print(ds.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4af937-f0fd-438b-a9e1-076c829784ea",
   "metadata": {},
   "source": [
    "##### **Sample a balanced subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8017185-40a6-4389-ba45-1fbe1f529296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Label distribution in 'train' split (dataset.DatasetDict):\n",
      "Category                                Count     Percentage\n",
      "------------------------------------------------------------\n",
      "0                                       1311      26.91%\n",
      "1                                       587       12.05%\n",
      "2                                       565       11.6%\n",
      "3                                       289       5.93%\n",
      "4                                       614       12.6%\n",
      "5                                       641       13.16%\n",
      "6                                       643       13.2%\n",
      "7                                       222       4.56%\n",
      "------------------------------------------------------------\n",
      "Total                                   4872      \n",
      "\n",
      "üîπ Label distribution in 'test' split (dataset.DatasetDict):\n",
      "Category                                Count     Percentage\n",
      "------------------------------------------------------------\n",
      "0                                       307       25.18%\n",
      "1                                       154       12.63%\n",
      "2                                       137       11.24%\n",
      "3                                       97        7.96%\n",
      "4                                       160       13.13%\n",
      "5                                       160       13.13%\n",
      "6                                       139       11.4%\n",
      "7                                       65        5.33%\n",
      "------------------------------------------------------------\n",
      "Total                                   1219      \n"
     ]
    }
   ],
   "source": [
    "from frugalai.utils.analytics import print_distribution\n",
    "\n",
    "print_distribution(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6fe9c-20a2-4700-99b0-7bf8f2a3eade",
   "metadata": {},
   "source": [
    "##### **Load tokenizer & model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce0edf9-fa30-49ec-afbb-add2d5f91cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unsloth/Phi-3.5-mini-instruct'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Model\n",
    "MODEL_NAMES = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"phi3\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"unsloth\": \"unsloth/Phi-3.5-mini-instruct\",\n",
    "    \"qwen\": \"Qwen/Qwen2.5-0.5B\"\n",
    "}\n",
    "selected_model = \"unsloth\"\n",
    "model_name = MODEL_NAMES[selected_model]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e353a59-4fbd-4eec-9b7b-cbbc6a1f3a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train': (4872, 2), 'test': (1219, 2)},\n",
       " {'train': ['quote', 'label'], 'test': ['quote', 'label']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.shape, ds.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75621470-8c36-41fe-bd0f-d191dbe52bcd",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcb824-96ca-47de-a749-a71b17b8e232",
   "metadata": {},
   "source": [
    "- essayer avec et sans padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a75a29-e304-4c07-ac24-7d75ceb95d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377b18ea-e36c-45d0-9fd8-56dbeff623d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341fc78be2fb40738cf64776423b5d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770928dafed541ba99a6f638f295a9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db53e35199244f269839a96d449f03bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cba5c86e254ee9a2b309082af3c8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba394ef749d64ca5b884825eb7370f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ FunctionTimer: load_tokenizer\n",
      "| time            00:00:08.4972\n",
      "| emissions       0.000000 CO2eq\n",
      "| energy consumed 0.000009 kWh\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b6c9600bf641f5b660f1b6e85540c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efde2525a9147f7824865e0244a75db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 4872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1219\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "@tracker.track\n",
    "def load_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_tokens)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "def preprocess_function(element):\n",
    "    return tokenizer(element[\"quote\"], truncation=True, max_length=max_tokens) #padding=\"max_length\"\n",
    "\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True, remove_columns=[\"quote\"])\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce343146-0b6e-4f5c-994f-46707e4e16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual max lenght in train set : 128 tokens\n",
      "Average sequence length: 62.2\n"
     ]
    }
   ],
   "source": [
    "train_lengths = [len(x) for x in tokenized_ds[\"train\"][\"input_ids\"]]\n",
    "print('Actual max lenght in train set :', max(train_lengths), 'tokens')\n",
    "print(f\"Average sequence length: {sum(train_lengths)/len(train_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b7955a-9a04-4cc1-a986-388645011e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8f71d-f9c3-4a7e-889f-b3c8a467edb5",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97a8871-e381-4168-a618-3521eadcacd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                    MEMORY USAGE REPORT                     \n",
      "============================================================\n",
      "\n",
      "-------------------- CPU MEMORY --------------------\n",
      "Total System Memory:         24.00 GB\n",
      "Available System Memory:     10.07 GB\n",
      "Used System Memory:          12.03 GB (58.0%)\n",
      "Current Process Memory:      0.50 GB\n",
      "\n",
      "-------------------- MPS MEMORY --------------------\n",
      "Tensor Allocated Memory:     0.00 GB\n",
      "Overhead (PyTorch Internal): 0.00 GB\n",
      "Driver Allocated Memory:     0.00 GB\n",
      "Recommended Maximum Memory:  16.00 GB\n",
      "Available in Memory Pool:    16.00 GB\n",
      "\n",
      "-------------------- TENSOR COUNTS --------------------\n",
      "CPU Tensors:                 0\n",
      "MPS Tensors:                 0\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.villa.massone/Library/Caches/pypoetry/virtualenvs/frugalai-BGGDvkbh-py3.12/lib/python3.12/site-packages/torch/__init__.py:1113: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    }
   ],
   "source": [
    "from frugalai.utils.monitoring import print_memory_status_across_devices\n",
    "\n",
    "print_memory_status_across_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16b48f-05dd-4fbe-8521-f811f5f1a082",
   "metadata": {},
   "source": [
    "- try with mps\n",
    "- if not, cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fce702-9287-4255-a94f-d2d98407b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 128 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # Can select any from the below:\n",
    "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "    # And also all Instruct versions and Math. Coding verisons!\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227eede3-af0c-40c2-b71c-a5604f680696",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdf40a-9fd6-4e76-a3f2-f085fd1930ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# For MPS compatibility, we need to be careful with quantization settings\n",
    "# MPS doesn't fully support all quantization formats, so we'll use bfloat16 for training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,        # Use double quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",             # Quantization type\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute datatype\n",
    ")\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde8617-cb26-4386-9f47-7e7341a061da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from frugalai.utils.hardware import get_device\n",
    "\n",
    "@tracker.track\n",
    "def load_model(model_name):\n",
    "\n",
    "    device = get_device() \n",
    "\n",
    "    if device.type == \"cuda\" or device.type == \"mps\":\n",
    "        dtype = torch.float16\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "\n",
    "    # Loading model with AutoModelForSequenceClassification adds an \n",
    "    # randomly initialized classification head : score.weight\n",
    "    # it will be trained along with the LoRa parameters during FT\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        num_labels=8,\n",
    "        #torch_dtype=dtype,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "        # trust_remote_code=True,\n",
    "        \n",
    "        # Load a quantized model\n",
    "        #load_in_8bit=True,  # Enable 8-bit quantization, allow auto device allocation in that case to help manage\n",
    "        \n",
    "        # Enable model sharding to optimize memory allocation\n",
    "        # device_map=auto, # for larger models : model sharding : auto distribution of model layers across available hardware, \n",
    "        # splits a large model across GPU and CPU. Handled by Accelerate library\n",
    "        \n",
    "    #.to(device) # for smaller models : moves the entire model to the specified device / all or nothing\n",
    "\n",
    "model = load_model(model_name)\n",
    "\n",
    "print('model.device :', model.device)\n",
    "print('precision model.dtype :', model.dtype)\n",
    "print('model.framework :',  model.framework)\n",
    "print('model.is_gradient_checkpointing :',  model.is_gradient_checkpointing)\n",
    "print('model.is_parallelizable :',  model.is_parallelizable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dcbbda-e839-442a-987c-afb31f6fe648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frugalai.utils.monitoring import print_memory_status_across_devices\n",
    "\n",
    "print_memory_status_across_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62be31-5c0c-4756-aeb3-354b0b3c3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80c174-ded1-42ce-b50c-682c441bbbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45533f42-df6b-4c6d-852f-2bbcbd5e7da8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b89fd9-6ed4-49d5-8b46-ea18154ce828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b666d65-e607-46b1-9ff3-de5ba934f8ae",
   "metadata": {},
   "source": [
    "**LoRa Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853e476-e11f-4041-8ce8-37094650e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ee2e6-0f73-4b0b-a660-4f05cfb8bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02e024-8b75-46e7-854a-8bb6fea7471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "modules = find_all_linear_names(model) \n",
    "    # target_modules=[\n",
    "    #    \"self_attn.qkv_proj\",\n",
    "    #    \"self_attn.o_proj\",\n",
    "    #    \"mlp.gate_up_proj\",\n",
    "    #    \"mlp.down_proj\"\n",
    "    #],\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=modules,\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(type(model))\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed4b85-8d2c-4c51-8c9f-a843efc3b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf540d5-5113-4485-9a66-fc7b64fb7291",
   "metadata": {},
   "source": [
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce528e-ea19-4f7d-9c3b-a26ac1913961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd608c-b4f6-403a-893b-80ed733e37c0",
   "metadata": {},
   "source": [
    "**Training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d58f37-d635-4127-851e-d7fbcc22ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def get_training_args(output_dir=\"./results\"):\n",
    "    return TrainingArguments(\n",
    "        max_steps=500,  # Adjust based on your dataset\n",
    "        warmup_steps=50,\n",
    "        eval_steps=50,\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,                         # Use mixed precision\n",
    "    optim=\"adamw_torch\",\n",
    "    label_names=['label'],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",                  # Disable reporting to wandb, etc.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    compute_metrics=compute_metrics\n",
    "    packing=True,                      # Enable packing for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4bef4-eaba-4acd-a1ba-21fe492b5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimated_time_per_step = 0.5\n",
    "#num_training_steps = (len(tokenized_ds['train']) // 4) * 5\n",
    "#total_training_time = estimated_time_per_step * num_training_steps \n",
    "\n",
    "#print(f\"Estimated training time: {total_training_time / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92a108-a427-4907-ad9e-9967047c36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frugalai.utils.monitoring import estimate_ft_memory_requirements\n",
    "\n",
    "memory_estimates = estimate_ft_memory_requirements(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    training_args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c727d9-5472-4c9a-8a43-72a3cbcb88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3aebfe-7792-4604-8952-8a596b87928c",
   "metadata": {},
   "source": [
    "**Do a small test run to check if it's ok**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7b511-f9bd-4777-8161-7352019afd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb942cf-b508-4c28-8754-7caa077c9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7327627-1769-4244-bd72-45b00e9267b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print validation accuracy\n",
    "print(f\"Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516c909-40c5-4626-9536-627c796278d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frugalai",
   "language": "python",
   "name": "frugalai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

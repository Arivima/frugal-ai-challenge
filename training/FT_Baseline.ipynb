{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72ddba8-599c-408c-bf51-94dd5ae894e8",
   "metadata": {},
   "source": [
    "### Evaluation LLM with HF Pipeline using HF datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb4d18-8e9d-4ffe-82d9-b03579981ccc",
   "metadata": {},
   "source": [
    "**Load dataset with HF datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfc79640-bc1e-43fe-9418-4976715326c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b04eec6b-26dc-422d-a1ed-dad98f0e7800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['quote', 'label', 'source', 'url', 'language', 'subsource', 'id', '__index_level_0__'],\n",
       "        num_rows: 4872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['quote', 'label', 'source', 'url', 'language', 'subsource', 'id', '__index_level_0__'],\n",
       "        num_rows: 1219\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8924fe6e-a8e2-4ce7-ad9f-6cf97a3b434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['quote', 'label'], 'test': ['quote', 'label']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.select_columns(['quote', 'label'])\n",
    "ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89185376-68bc-482c-aca4-967ef9a89efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in 'train' split:\n",
      "Label\tCount\tPercentage\n",
      "0\t1311\t26.91%\n",
      "1\t587\t12.05%\n",
      "2\t565\t11.6%\n",
      "3\t289\t5.93%\n",
      "4\t614\t12.6%\n",
      "5\t641\t13.16%\n",
      "6\t643\t13.2%\n",
      "7\t222\t4.56%\n",
      "\n",
      "Label distribution in 'test' split:\n",
      "Label\tCount\tPercentage\n",
      "0\t307\t25.18%\n",
      "1\t154\t12.63%\n",
      "2\t137\t11.24%\n",
      "3\t97\t7.96%\n",
      "4\t160\t13.13%\n",
      "5\t160\t13.13%\n",
      "6\t139\t11.4%\n",
      "7\t65\t5.33%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each class label\n",
    "def print_class_distribution(dataset):\n",
    "    for split in dataset.keys():\n",
    "        label_counts = Counter(dataset[split][\"label\"])\n",
    "        total = sum(label_counts.values())\n",
    "        \n",
    "        print(f\"\\nLabel distribution in '{split}' split:\")\n",
    "        print(\"Label\\tCount\\tPercentage\")\n",
    "        for label, count in sorted(label_counts.items()):\n",
    "            percentage = round((count / total) * 100, 2)\n",
    "            print(f\"{label.split('_')[0]}\\t{count}\\t{percentage}%\")\n",
    "\n",
    "print_class_distribution(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd4fc8c-6a81-41e9-9d19-2f91075e784e",
   "metadata": {},
   "source": [
    "**Subset with balanced classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9650b557-7753-4933-a1c9-6417e087bece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['quote', 'label', 'trunc_quote'],\n",
      "        num_rows: 48\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['quote', 'label', 'trunc_quote'],\n",
      "        num_rows: 48\n",
      "    })\n",
      "})\n",
      "\n",
      "Label distribution in 'train' split:\n",
      "Label\tCount\tPercentage\n",
      "0\t6\t12.5%\n",
      "1\t6\t12.5%\n",
      "2\t6\t12.5%\n",
      "3\t6\t12.5%\n",
      "4\t6\t12.5%\n",
      "5\t6\t12.5%\n",
      "6\t6\t12.5%\n",
      "7\t6\t12.5%\n",
      "\n",
      "Label distribution in 'test' split:\n",
      "Label\tCount\tPercentage\n",
      "0\t6\t12.5%\n",
      "1\t6\t12.5%\n",
      "2\t6\t12.5%\n",
      "3\t6\t12.5%\n",
      "4\t6\t12.5%\n",
      "5\t6\t12.5%\n",
      "6\t6\t12.5%\n",
      "7\t6\t12.5%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import DatasetDict, Dataset, concatenate_datasets\n",
    "\n",
    "# Function to sample data while maintaining balance\n",
    "def sample_balanced_subset(dataset, N=50, seed=42):\n",
    "    random.seed(seed)\n",
    "    \n",
    "    labels = dataset.unique(\"label\")\n",
    "    num_classes = len(labels)\n",
    "    samples_per_class = N // num_classes\n",
    "    \n",
    "    subset = []\n",
    "    \n",
    "    for label in labels:\n",
    "        samples = dataset.filter(lambda x: x[\"label\"] == label)\n",
    "        shuffled_samples = samples.shuffle(seed=seed)\n",
    "        selection = shuffled_samples.select(range(min(samples_per_class, len(samples))))\n",
    "        \n",
    "        subset.append(selection)\n",
    "    \n",
    "    return concatenate_datasets(subset)\n",
    "\n",
    "subset_ds = DatasetDict({split: sample_balanced_subset(ds[split]) for split in ds.keys()})\n",
    "\n",
    "print(subset_ds)\n",
    "print_class_distribution(subset_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884f732-ff3d-44fc-a00a-2ceb8f6e8ef0",
   "metadata": {},
   "source": [
    "**Truncate sentence lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "761a7118-0375-4cbe-a701-9c9c62a37242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554af52dbf7b45b6a6a700cf2b4b7ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f191ff56e61b4e86b8e88a6db4df6782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': ['quote', 'label', 'trunc_quote'],\n",
       " 'test': ['quote', 'label', 'trunc_quote']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 750\n",
    "\n",
    "# Add a truncated quote column\n",
    "def add_truncated_quote(example):\n",
    "    example[\"trunc_quote\"] = example[\"quote\"][:threshold]\n",
    "    return example\n",
    "\n",
    "subset_ds = subset_ds.map(add_truncated_quote)\n",
    "\n",
    "subset_ds.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf3d66-1037-49cb-b49d-47daa76b2f4f",
   "metadata": {},
   "source": [
    "**Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ce0edf9-fa30-49ec-afbb-add2d5f91cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/Phi-3-mini-4k-instruct'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Model\n",
    "MODEL_NAMES = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"phi3\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"qwen\": \"Qwen/Qwen2.5-0.5B\"\n",
    "}\n",
    "selected_model = \"phi3\"\n",
    "model_name = MODEL_NAMES[selected_model]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7e90a7-70d4-4877-bd58-4e1786add58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f254fde-80df-4440-b8b1-07df512a131d",
   "metadata": {},
   "source": [
    "- LLM type mistral 7B phi 3\n",
    "- en local\n",
    "- multiclass\n",
    "- Langchain-hugginface - pas de text-classification\n",
    "- input as embedding vs input as prompt\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e71f635-933d-4d84-b8cb-7ed9cac63305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "        pipeline_kwargs={\n",
    "            \"max_new_tokens\": 2, #1\n",
    "            \"top_k\": 50,\n",
    "            \"temperature\": 0.1,\n",
    "            #\"device_map\":\"auto\",\n",
    "            #\"batch_size\": 8,  # Process 8 quotes at a time\n",
    "            #\"do_sample\":False,\n",
    "            #repetition_penalty=1.03,\n",
    "            \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# pipeline = pipeline(\"text-classification\", model=model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30fc5c79-1425-43d4-8b26-8eea8de9fffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e849be51bb4d45b9daffcc4f9d80f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_7', 'score': 0.6066178679466248}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text_classifier = pipeline(\n",
    "    task=\"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "sequence = \"Who are you voting for in 2020?\"\n",
    "candidate_labels = [\"politics\", \"public health\", \"economics\", \"elections\"]\n",
    "hypothesis_template = \"The sentiment of this review is {}.\"\n",
    "text_classifier(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2805bf26-e607-4c10-b285-879cdc6e7760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4871ae6aff4a7796212e775c763429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following text into one of these 8 categories: politics, health, economy, technology, sports, science, entertainment, education.\n",
      "\n",
      "Text: The government announced new economic policies today.\n",
      "Category: economy\n",
      "\n",
      "Text: A new study shows that\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Choose any LLM like Mistral 7B or Phi-3\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"  # Change to \"mistralai/Mistral-7B-Instruct\" if needed\n",
    "\n",
    "# Load text-generation pipeline\n",
    "text_classifier = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model_name,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=10  # Limit output to a short classification response\n",
    ")\n",
    "\n",
    "# Define a general prompt (keeps it model-agnostic)\n",
    "def classify_text(text):\n",
    "    prompt = f\"Classify the following text into one of these 8 categories: politics, health, economy, technology, sports, science, entertainment, education.\\n\\nText: {text}\\nCategory:\"\n",
    "    response = text_classifier(prompt)\n",
    "    return response[0][\"generated_text\"]\n",
    "\n",
    "# Example usage\n",
    "text = \"The government announced new economic policies today.\"\n",
    "print(classify_text(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8736d7-7a4f-407a-b2dd-cc2f26d2e26c",
   "metadata": {},
   "source": [
    "**Zero shot pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22e660e4-f2c6-4a71-b2cf-7b94e285f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Who are you voting for in 2020?',\n",
       " 'labels': ['elections', 'politics', 'public health', 'economics'],\n",
       " 'scores': [0.9914352297782898,\n",
       "  0.9716799259185791,\n",
       "  0.03402749449014664,\n",
       "  0.001957494532689452]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "sequence = \"Who are you voting for in 2020?\"\n",
    "candidate_labels = [\"politics\", \"public health\", \"economics\", \"elections\"]\n",
    "hypothesis_template = \"The sentiment of this review is {}.\"\n",
    "\n",
    "classifier(sequence, candidate_labels, multi_label=True, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b7402f-b61c-4c0c-b03a-7966dbd35127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mmodels--Qwen--Qwen2.5-0.5B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--Qwen--Qwen2.5-7B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--microsoft--Phi-3-mini-4k-instruct\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--mistralai--Mistral-7B-Instruct-v0.1\u001b[m\u001b[m\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048a57fb-6d1f-4e32-98d8-a24e9148739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1G\t/Users/a.villa.massone/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct\n"
     ]
    }
   ],
   "source": [
    "!du -sh ~/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd945eb-7c59-4959-b736-979a36b36794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 3.82 Billion\n",
      "Precision: torch.float32\n",
      "Estimated memory needed: 15.28 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def print_model_info(model=None, model_name=None):\n",
    "    # Load tokenizer and model to inspect parameters\n",
    "    if model is None and model_name is None:\n",
    "        return None\n",
    "    llm = AutoModelForCausalLM.from_pretrained(model_name) if model is None else model.llm.pipeline.model\n",
    "    \n",
    "    # Compute total parameters\n",
    "    total_params = sum(p.numel() for p in llm.parameters())\n",
    "    print(f\"Total Parameters: {total_params / 1e9:.2f} Billion\")\n",
    "\n",
    "    # Check precision\n",
    "    precision = next(llm.parameters()).dtype\n",
    "    print(f\"Precision: {precision}\")\n",
    "\n",
    "    # Estimate memory requirement\n",
    "    bits_per_param = {\n",
    "        torch.float32: 4,  # FP32 = 4 bytes per parameter\n",
    "        torch.float16: 2,  # FP16 = 2 bytes per parameter\n",
    "        torch.bfloat16: 2, # BF16 = 2 bytes per parameter\n",
    "        torch.int8: 1,     # INT8 = 1 byte per parameter\n",
    "        torch.int4: 0.5    # 4-bit quantization\n",
    "    }\n",
    "    \n",
    "    # Get memory per parameter in bytes\n",
    "    memory_per_param = bits_per_param.get(precision, 4)  # Default FP32 if unknown\n",
    "    estimated_memory_gb = (total_params * memory_per_param) / 1e9  # Convert bytes to GB\n",
    "    \n",
    "    print(f\"Estimated memory needed: {estimated_memory_gb:.2f} GB\")\n",
    "    \n",
    "    if model is None:\n",
    "        del model\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    return estimated_memory_gb\n",
    "    \n",
    "estimated_memory_gb = print_model_info(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6f5b86a-1ea4-45ea-858d-18d44fd6b08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated by MPS: 15.28 GB\n",
      "Allocated by driver: 17.26 GB\n",
      "Available system memory: 4.78 GB\n",
      "Cache cleared.\n",
      "Allocated by MPS: 15.28 GB\n",
      "Allocated by driver: 16.12 GB\n",
      "Available system memory: 5.34 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "def cache_info(empty_cache=False):\n",
    "    def print_cache():\n",
    "        current = torch.mps.current_allocated_memory() / 1e9\n",
    "        driver = torch.mps.driver_allocated_memory() / 1e9\n",
    "        available_memory = psutil.virtual_memory().available / 1e9 \n",
    "\n",
    "        print(\"Allocated by MPS:\", round(current, 2), \"GB\")\n",
    "        print(\"Allocated by driver:\", round(driver, 2), \"GB\")\n",
    "        print(\"Available system memory:\", round(available_memory, 2), \"GB\")\n",
    "\n",
    "    print_cache()\n",
    "    if empty_cache:\n",
    "        torch.mps.empty_cache()  # Clears PyTorch's unused memory\n",
    "        torch.mps.synchronize()  # Ensures all pending ops are done\n",
    "        print(\"Cache cleared.\")\n",
    "        print_cache()\n",
    "\n",
    "# Example usage\n",
    "cache_info(empty_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a149fc7-8669-4bad-8b4f-651f72218d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.villa.massone/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nHugging Face is<|end|>\\n<|assistant|>\\n Hug'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"Hugging Face is\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad4729e-87ca-4693-a0e7-bf790abce817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<|system|>\\nYou're a helpful assistant<|end|>\\n<|user|>\\nWhat happens when an unstoppable force meets an immovable object?<|end|>\\n<|assistant|>\\n This is\", additional_kwargs={}, response_metadata={}, id='run-0fc0e8f3-e3b4-4bb1-8852-6b1439314954-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = model.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73511614-aa5b-404f-9f72-2286b19ba401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sys_msg():\n",
    "    return f\"\"\"\n",
    "<instruction>\n",
    "Classify the following statement into one of these 8 categories:\n",
    "Respond STRICTLY with only the corresponding number.\n",
    "If you do not know the answer, respond \"?\".\n",
    "</instruction>\n",
    "\n",
    "<categories>\n",
    "0 - Not relevant: No climate-related claims or doesn't fit other categories\n",
    "1 - Denial: Claims climate change is not happening\n",
    "2 - Attribution denial: Claims human activity is not causing climate change\n",
    "3 - Impact minimization: Claims climate change impacts are minimal or beneficial\n",
    "4 - Solution opposition: Claims solutions to climate change are harmful\n",
    "5 - Science skepticism: Challenges climate science validity or methods\n",
    "6 - Actor criticism: Attacks credibility of climate scientists or activists\n",
    "7 - Fossil fuel promotion: Asserts importance of fossil fuels\n",
    "</categories>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a443fb8-2e09-46cd-beda-a6aae64bda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_human_msg(text):\n",
    "    return f\"\"\"\n",
    "<statement>\n",
    "Statement: \"{text}\"\n",
    "</statement>\n",
    "\n",
    "Category number:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab0055f-013d-478f-a0f3-d8173614174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def apply_chat_template(text):\n",
    "    messages = [\n",
    "        SystemMessage(content=create_sys_msg()),\n",
    "        HumanMessage(\n",
    "            content=create_human_msg(text)\n",
    "        ),\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037b56e2-5f47-4fc5-943a-c0d965280876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_response(response):\n",
    "    text = response.content\n",
    "    parts = text.split(\"<|assistant|>\")\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Example usage\n",
    "#clean_response = extract_assistant_response(ai_msg)\n",
    "# print(clean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147ac1e8-8905-4bbf-b991-c83070bf5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Labels\n",
    "CLASS_LABELS = [\n",
    "    \"0_not_relevant\", \"1_not_happening\", \"2_not_human\", \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\", \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\", \"7_fossil_fuels_needed\"\n",
    "]\n",
    "\n",
    "def parse_output(response):\n",
    "    if response.isdigit() and int(response) in range(8):\n",
    "        return CLASS_LABELS[int(response)]\n",
    "    if response == '?':\n",
    "        return \"unknown\"\n",
    "    return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9592ebec-1f2a-4518-81d5-48b52770db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann could be said to be the Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science that could have dire economic consequences for the nation and planet,” Rand Simberg wrote in National Review article in 2012. 6_proponents_biased\n",
      "content='<|system|>\\n\\n<instruction>\\nClassify the following statement into one of these 8 categories:\\nRespond STRICTLY with only the corresponding number.\\nIf you do not know the answer, respond \"?\".\\n</instruction>\\n\\n<categories>\\n0 - Not relevant: No climate-related claims or doesn\\'t fit other categories\\n1 - Denial: Claims climate change is not happening\\n2 - Attribution denial: Claims human activity is not causing climate change\\n3 - Impact minimization: Claims climate change impacts are minimal or beneficial\\n4 - Solution opposition: Claims solutions to climate change are harmful\\n5 - Science skepticism: Challenges climate science validity or methods\\n6 - Actor criticism: Attacks credibility of climate scientists or activists\\n7 - Fossil fuel promotion: Asserts importance of fossil fuels\\n</categories>\\n<|end|>\\n<|user|>\\n\\n<statement>\\nStatement: \"Mann could be said to be the Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science that could have dire economic consequences for the nation and planet,” Rand Simberg wrote in\\xa0National Review\\xa0article in\\xa02012.\"\\n</statement>\\n\\nCategory number:<|end|>\\n<|assistant|>\\n 6' additional_kwargs={} response_metadata={} id='run-95ed5cfc-6202-447c-97f5-7362be865ff0-0'\n",
      "6\n",
      "6_proponents_biased\n"
     ]
    }
   ],
   "source": [
    "# test for one quote\n",
    "for x, y in zip(X_test, y_test):\n",
    "    quote = x\n",
    "    label = y\n",
    "    break\n",
    "print(quote, label)\n",
    "\n",
    "response = model.invoke(apply_chat_template(quote))\n",
    "print(response)\n",
    "assistant_response = extract_assistant_response(response)\n",
    "print(assistant_response)\n",
    "output = parse_output(assistant_response)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8881734a-b39e-4a5d-830a-6785cea7e16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6386    [content='\\n<instruction>\\nClassify the follow...\n",
       "1612    [content='\\n<instruction>\\nClassify the follow...\n",
       "1718    [content='\\n<instruction>\\nClassify the follow...\n",
       "561     [content='\\n<instruction>\\nClassify the follow...\n",
       "5634    [content='\\n<instruction>\\nClassify the follow...\n",
       "Name: quote, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare all quotes\n",
    "chat_msgs = X_test.apply(apply_chat_template)\n",
    "chat_msgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0bba9-9ccd-4419-9cb8-20fb699ed7b6",
   "metadata": {},
   "source": [
    "**comment accelerer le batch inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd457cc-3d23-477f-891d-db75d4207151",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def classify_batch(quotes, model):    \n",
    "    responses = model.batch(list(quotes))\n",
    "    prediction = [r.content for r in responses]\n",
    "    return pd.Series(responses)\n",
    "\n",
    "# Example usage\n",
    "responses = classify_batch(chat_msgs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4a593b9-6da8-4f78-a180-3ddbe7699540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'langchain_core.messages.ai.AIMessage'> 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|system|>\\n\\n<instruction>\\nClassify the following statement into one of these 8 categories:\\nRespond STRICTLY with only the corresponding number.\\nIf you do not know the answer, respond \"?\".\\n</instruction>\\n\\n<categories>\\n0 - Not relevant: No climate-related claims or doesn\\'t fit other categories\\n1 - Denial: Claims climate change is not happening\\n2 - Attribution denial: Claims human activity is not causing climate change\\n3 - Impact minimization: Claims climate change impacts are minimal or beneficial\\n4 - Solution opposition: Claims solutions to climate change are harmful\\n5 - Science skepticism: Challenges climate science validity or methods\\n6 - Actor criticism: Attacks credibility of climate scientists or activists\\n7 - Fossil fuel promotion: Asserts importance of fossil fuels\\n</categories>\\n<|end|>\\n<|user|>\\n\\n<statement>\\nStatement: \"Mann could be said to be the Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science that could have dire economic consequences for the nation and planet,” Rand Simberg wrote in\\xa0National Review\\xa0article in\\xa02012.\"\\n</statement>\\n\\nCategory number:<|end|>\\n<|assistant|>\\n 6', additional_kwargs={}, response_metadata={}, id='run-59010fb0-6251-4cf0-b135-086e4e5eb989-0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(responses), type(responses[0]), len(responses))\n",
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d332de25-51b1-4a45-a423-64f8f3f4b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_response(response : str):\n",
    "    parts = response.split(\"<|assistant|>\")\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13e5c8d5-7d3f-46cf-bb52-71a34669af39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 6_proponents_biased\n",
       "1                5_science_unreliable\n",
       "2                     1_not_happening\n",
       "3                5_science_unreliable\n",
       "4                     1_not_happening\n",
       "5               7_fossil_fuels_needed\n",
       "6                           3_not_bad\n",
       "7                 6_proponents_biased\n",
       "8                     1_not_happening\n",
       "9                         2_not_human\n",
       "10               5_science_unreliable\n",
       "11               5_science_unreliable\n",
       "12               5_science_unreliable\n",
       "13              7_fossil_fuels_needed\n",
       "14                          3_not_bad\n",
       "15                6_proponents_biased\n",
       "16                          3_not_bad\n",
       "17                6_proponents_biased\n",
       "18    4_solutions_harmful_unnecessary\n",
       "19               5_science_unreliable\n",
       "20              7_fossil_fuels_needed\n",
       "21                6_proponents_biased\n",
       "22               5_science_unreliable\n",
       "23                          3_not_bad\n",
       "24                              error\n",
       "25                6_proponents_biased\n",
       "26                     0_not_relevant\n",
       "27                              error\n",
       "28                              error\n",
       "29               5_science_unreliable\n",
       "30                              error\n",
       "31                              error\n",
       "32               5_science_unreliable\n",
       "33                              error\n",
       "34                6_proponents_biased\n",
       "35                              error\n",
       "36                              error\n",
       "37                              error\n",
       "38                              error\n",
       "39                              error\n",
       "40                              error\n",
       "41                              error\n",
       "42                              error\n",
       "43                              error\n",
       "44                              error\n",
       "45                              error\n",
       "46                              error\n",
       "47                              error\n",
       "48              7_fossil_fuels_needed\n",
       "49                6_proponents_biased\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = [r.content for r in responses]\n",
    "responses_s = pd.Series(prediction)\n",
    "assistant_responses = responses_s.apply(extract_assistant_response)\n",
    "y_pred = assistant_responses.apply(parse_output)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c5e0712-3680-4310-8ace-12662315af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluation(X_test, y_test, y_pred):\n",
    "    # Store results in a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "    results[\"correct\"] = results[\"y_test\"] == results[\"y_pred\"]\n",
    "\n",
    "    # Compute overall performance breakdown\n",
    "    correct = np.sum(results[\"correct\"])\n",
    "    unknown = np.sum(results[\"y_pred\"] == 'unknown')\n",
    "    errors = np.sum(results[\"y_pred\"] == 'error')\n",
    "    incorrect = len(results) - correct - errors\n",
    "    \n",
    "    performance = pd.DataFrame({\n",
    "        'Outcome': ['Correct', 'Incorrect', 'Unknown', 'Error'],\n",
    "        'Count': [correct,incorrect,unknown, errors]\n",
    "        })\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get category labels (sorted for consistency)\n",
    "    category_names = sorted(pd.Series(y_test).unique())\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    class_accuracy = results.groupby(\"y_test\")[\"correct\"].mean().reindex(category_names, fill_value=0).values\n",
    "    precision = precision_score(y_test, y_pred, average=None, labels=category_names, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None, labels=category_names, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=None, labels=category_names, zero_division=0)\n",
    "\n",
    "    # Store per-category metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Category\": category_names,\n",
    "        \"Accuracy\": class_accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "\n",
    "    return results, accuracy, metrics_df, performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1927a12c-4ba9-43fa-b578-7764c0f312f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cd14c47-a0e4-4d33-a8ca-252aef25bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, accuracy, metrics_df, performance = evaluation(X_test.tolist(), y_test.tolist(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc666645-d00b-40d5-bfb0-cab57e0bb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors : 0 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"errors : {round(performance.iloc[2]['Count'] / sum(performance['Count']) * 100)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea36b21f-620c-425b-9013-2b128823171a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Correct</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Error</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outcome  Count\n",
       "0    Correct     15\n",
       "1  Incorrect     16\n",
       "2    Unknown      0\n",
       "3      Error     19"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ebb731-8440-4c93-afc8-0fba4b8232b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397eeed9-1290-4c5e-8260-31baef5603e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frugalai-BGGDvkbh-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

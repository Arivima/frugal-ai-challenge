{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72ddba8-599c-408c-bf51-94dd5ae894e8",
   "metadata": {},
   "source": [
    "### Evaluation LLM with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e02ecd-8b10-4e4f-8f62-d37b867f23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/QuotaClimat/frugalaichallenge-text-train/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8924fe6e-a8e2-4ce7-ad9f-6cf97a3b434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['quote', 'label'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['quote', 'label']]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "761a7118-0375-4cbe-a701-9c9c62a37242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['quote', 'label', 'truncated_quote'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 750\n",
    "df['truncated_quote'] = df['quote'].str.slice(0, int(threshold))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ffacca-e060-491f-b00f-e5b307ac4aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50,),\n",
       " (50,),\n",
       " label\n",
       " 5_science_unreliable               0.18\n",
       " 3_not_bad                          0.16\n",
       " 6_proponents_biased                0.14\n",
       " 2_not_human                        0.12\n",
       " 0_not_relevant                     0.12\n",
       " 1_not_happening                    0.10\n",
       " 4_solutions_harmful_unnecessary    0.10\n",
       " 7_fossil_fuels_needed              0.08\n",
       " Name: proportion, dtype: float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "N_SAMPLES = 50\n",
    "\n",
    "df_test = df.sample(N_SAMPLES, random_state=42)\n",
    "X_test = df_test['quote']\n",
    "y_test = df_test['label']\n",
    "X_test.shape, y_test.shape, y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ce0edf9-fa30-49ec-afbb-add2d5f91cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/Phi-3-mini-4k-instruct'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Model\n",
    "MODEL_NAMES = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"phi3\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"qwen\": \"Qwen/Qwen2.5-0.5B\"\n",
    "}\n",
    "selected_model = \"phi3\"\n",
    "model_name = MODEL_NAMES[selected_model]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7e90a7-70d4-4877-bd58-4e1786add58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d8eb12-2ce4-4ba3-848f-cf421bfc8a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f2d6997e67404b9b0937826a490b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "def load_model(model_name):\n",
    "    torch.mps.empty_cache()\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=model_name,\n",
    "        task=\"text-generation\",\n",
    "        pipeline_kwargs={\n",
    "            \"max_new_tokens\": 2, #1\n",
    "            \"top_k\": 50,\n",
    "            \"temperature\": 0.1,\n",
    "            #\"device_map\":\"auto\",\n",
    "            #\"batch_size\": 8,  # Process 8 quotes at a time\n",
    "            #\"do_sample\":False,\n",
    "            #repetition_penalty=1.03,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    llm_engine = ChatHuggingFace(llm=llm)\n",
    "\n",
    "    return llm_engine\n",
    "\n",
    "model = load_model(model_name)\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b7402f-b61c-4c0c-b03a-7966dbd35127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mmodels--Qwen--Qwen2.5-0.5B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--Qwen--Qwen2.5-7B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--microsoft--Phi-3-mini-4k-instruct\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mmodels--mistralai--Mistral-7B-Instruct-v0.1\u001b[m\u001b[m\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048a57fb-6d1f-4e32-98d8-a24e9148739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1G\t/Users/a.villa.massone/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct\n"
     ]
    }
   ],
   "source": [
    "!du -sh ~/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd945eb-7c59-4959-b736-979a36b36794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 3.82 Billion\n",
      "Precision: torch.float32\n",
      "Estimated memory needed: 15.28 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def print_model_info(model=None, model_name=None):\n",
    "    # Load tokenizer and model to inspect parameters\n",
    "    if model is None and model_name is None:\n",
    "        return None\n",
    "    llm = AutoModelForCausalLM.from_pretrained(model_name) if model is None else model.llm.pipeline.model\n",
    "    \n",
    "    # Compute total parameters\n",
    "    total_params = sum(p.numel() for p in llm.parameters())\n",
    "    print(f\"Total Parameters: {total_params / 1e9:.2f} Billion\")\n",
    "\n",
    "    # Check precision\n",
    "    precision = next(llm.parameters()).dtype\n",
    "    print(f\"Precision: {precision}\")\n",
    "\n",
    "    # Estimate memory requirement\n",
    "    bits_per_param = {\n",
    "        torch.float32: 4,  # FP32 = 4 bytes per parameter\n",
    "        torch.float16: 2,  # FP16 = 2 bytes per parameter\n",
    "        torch.bfloat16: 2, # BF16 = 2 bytes per parameter\n",
    "        torch.int8: 1,     # INT8 = 1 byte per parameter\n",
    "        torch.int4: 0.5    # 4-bit quantization\n",
    "    }\n",
    "    \n",
    "    # Get memory per parameter in bytes\n",
    "    memory_per_param = bits_per_param.get(precision, 4)  # Default FP32 if unknown\n",
    "    estimated_memory_gb = (total_params * memory_per_param) / 1e9  # Convert bytes to GB\n",
    "    \n",
    "    print(f\"Estimated memory needed: {estimated_memory_gb:.2f} GB\")\n",
    "    \n",
    "    if model is None:\n",
    "        del model\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "    return estimated_memory_gb\n",
    "    \n",
    "estimated_memory_gb = print_model_info(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6f5b86a-1ea4-45ea-858d-18d44fd6b08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated by MPS: 15.28 GB\n",
      "Allocated by driver: 17.26 GB\n",
      "Available system memory: 4.78 GB\n",
      "Cache cleared.\n",
      "Allocated by MPS: 15.28 GB\n",
      "Allocated by driver: 16.12 GB\n",
      "Available system memory: 5.34 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "\n",
    "def cache_info(empty_cache=False):\n",
    "    def print_cache():\n",
    "        current = torch.mps.current_allocated_memory() / 1e9\n",
    "        driver = torch.mps.driver_allocated_memory() / 1e9\n",
    "        available_memory = psutil.virtual_memory().available / 1e9 \n",
    "\n",
    "        print(\"Allocated by MPS:\", round(current, 2), \"GB\")\n",
    "        print(\"Allocated by driver:\", round(driver, 2), \"GB\")\n",
    "        print(\"Available system memory:\", round(available_memory, 2), \"GB\")\n",
    "\n",
    "    print_cache()\n",
    "    if empty_cache:\n",
    "        torch.mps.empty_cache()  # Clears PyTorch's unused memory\n",
    "        torch.mps.synchronize()  # Ensures all pending ops are done\n",
    "        print(\"Cache cleared.\")\n",
    "        print_cache()\n",
    "\n",
    "# Example usage\n",
    "cache_info(empty_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a149fc7-8669-4bad-8b4f-651f72218d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.villa.massone/miniconda3/envs/frugal-notebooks-env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nHugging Face is<|end|>\\n<|assistant|>\\n Hug'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"Hugging Face is\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad4729e-87ca-4693-a0e7-bf790abce817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<|system|>\\nYou're a helpful assistant<|end|>\\n<|user|>\\nWhat happens when an unstoppable force meets an immovable object?<|end|>\\n<|assistant|>\\n This is\", additional_kwargs={}, response_metadata={}, id='run-0fc0e8f3-e3b4-4bb1-8852-6b1439314954-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(\n",
    "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "ai_msg = model.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73511614-aa5b-404f-9f72-2286b19ba401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sys_msg():\n",
    "    return f\"\"\"\n",
    "<instruction>\n",
    "Classify the following statement into one of these 8 categories:\n",
    "Respond STRICTLY with only the corresponding number.\n",
    "If you do not know the answer, respond \"?\".\n",
    "</instruction>\n",
    "\n",
    "<categories>\n",
    "0 - Not relevant: No climate-related claims or doesn't fit other categories\n",
    "1 - Denial: Claims climate change is not happening\n",
    "2 - Attribution denial: Claims human activity is not causing climate change\n",
    "3 - Impact minimization: Claims climate change impacts are minimal or beneficial\n",
    "4 - Solution opposition: Claims solutions to climate change are harmful\n",
    "5 - Science skepticism: Challenges climate science validity or methods\n",
    "6 - Actor criticism: Attacks credibility of climate scientists or activists\n",
    "7 - Fossil fuel promotion: Asserts importance of fossil fuels\n",
    "</categories>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a443fb8-2e09-46cd-beda-a6aae64bda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_human_msg(text):\n",
    "    return f\"\"\"\n",
    "<statement>\n",
    "Statement: \"{text}\"\n",
    "</statement>\n",
    "\n",
    "Category number:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab0055f-013d-478f-a0f3-d8173614174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def apply_chat_template(text):\n",
    "    messages = [\n",
    "        SystemMessage(content=create_sys_msg()),\n",
    "        HumanMessage(\n",
    "            content=create_human_msg(text)\n",
    "        ),\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "037b56e2-5f47-4fc5-943a-c0d965280876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_response(response):\n",
    "    text = response.content\n",
    "    parts = text.split(\"<|assistant|>\")\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Example usage\n",
    "#clean_response = extract_assistant_response(ai_msg)\n",
    "# print(clean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147ac1e8-8905-4bbf-b991-c83070bf5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Labels\n",
    "CLASS_LABELS = [\n",
    "    \"0_not_relevant\", \"1_not_happening\", \"2_not_human\", \"3_not_bad\",\n",
    "    \"4_solutions_harmful_unnecessary\", \"5_science_unreliable\",\n",
    "    \"6_proponents_biased\", \"7_fossil_fuels_needed\"\n",
    "]\n",
    "\n",
    "def parse_output(response):\n",
    "    if response.isdigit() and int(response) in range(8):\n",
    "        return CLASS_LABELS[int(response)]\n",
    "    if response == '?':\n",
    "        return \"unknown\"\n",
    "    return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9592ebec-1f2a-4518-81d5-48b52770db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann could be said to be the Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science that could have dire economic consequences for the nation and planet,” Rand Simberg wrote in National Review article in 2012. 6_proponents_biased\n",
      "content='<|system|>\\n\\n<instruction>\\nClassify the following statement into one of these 8 categories:\\nRespond STRICTLY with only the corresponding number.\\nIf you do not know the answer, respond \"?\".\\n</instruction>\\n\\n<categories>\\n0 - Not relevant: No climate-related claims or doesn\\'t fit other categories\\n1 - Denial: Claims climate change is not happening\\n2 - Attribution denial: Claims human activity is not causing climate change\\n3 - Impact minimization: Claims climate change impacts are minimal or beneficial\\n4 - Solution opposition: Claims solutions to climate change are harmful\\n5 - Science skepticism: Challenges climate science validity or methods\\n6 - Actor criticism: Attacks credibility of climate scientists or activists\\n7 - Fossil fuel promotion: Asserts importance of fossil fuels\\n</categories>\\n<|end|>\\n<|user|>\\n\\n<statement>\\nStatement: \"Mann could be said to be the Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science that could have dire economic consequences for the nation and planet,” Rand Simberg wrote in\\xa0National Review\\xa0article in\\xa02012.\"\\n</statement>\\n\\nCategory number:<|end|>\\n<|assistant|>\\n 6' additional_kwargs={} response_metadata={} id='run-95ed5cfc-6202-447c-97f5-7362be865ff0-0'\n",
      "6\n",
      "6_proponents_biased\n"
     ]
    }
   ],
   "source": [
    "# test for one quote\n",
    "for x, y in zip(X_test, y_test):\n",
    "    quote = x\n",
    "    label = y\n",
    "    break\n",
    "print(quote, label)\n",
    "\n",
    "response = model.invoke(apply_chat_template(quote))\n",
    "print(response)\n",
    "assistant_response = extract_assistant_response(response)\n",
    "print(assistant_response)\n",
    "output = parse_output(assistant_response)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8881734a-b39e-4a5d-830a-6785cea7e16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6386    [content='\\n<instruction>\\nClassify the follow...\n",
       "1612    [content='\\n<instruction>\\nClassify the follow...\n",
       "1718    [content='\\n<instruction>\\nClassify the follow...\n",
       "561     [content='\\n<instruction>\\nClassify the follow...\n",
       "5634    [content='\\n<instruction>\\nClassify the follow...\n",
       "Name: quote, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare all quotes\n",
    "chat_msgs = X_test.apply(apply_chat_template)\n",
    "chat_msgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0bba9-9ccd-4419-9cb8-20fb699ed7b6",
   "metadata": {},
   "source": [
    "**comment accelerer le batch inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd457cc-3d23-477f-891d-db75d4207151",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def classify_batch(quotes, model):    \n",
    "    responses = model.batch(list(quotes))\n",
    "    prediction = [r.content for r in responses]\n",
    "    return pd.Series(responses)\n",
    "\n",
    "# Example usage\n",
    "responses = classify_batch(chat_msgs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4a593b9-6da8-4f78-a180-3ddbe7699540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'langchain_core.messages.ai.AIMessage'> 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<|system|>\\n\\n<instruction>\\nClassify the following statement into one of these 8 categories:\\nRespond STRICTLY with only the corresponding number.\\nIf you do not know the answer, respond \"?\".\\n</instruction>\\n\\n<categories>\\n0 - Not relevant: No climate-related claims or doesn\\'t fit other categories\\n1 - Denial: Claims climate change is not happening\\n2 - Attribution denial: Claims human activity is not causing climate change\\n3 - Impact minimization: Claims climate change impacts are minimal or beneficial\\n4 - Solution opposition: Claims solutions to climate change are harmful\\n5 - Science skepticism: Challenges climate science validity or methods\\n6 - Actor criticism: Attacks credibility of climate scientists or activists\\n7 - Fossil fuel promotion: Asserts importance of fossil fuels\\n</categories>\\n<|end|>\\n<|user|>\\n\\n<statement>\\nStatement: \"Mann could be said to be the Jerry Sandusky of climate science, except for instead of molesting children, he has molested and tortured data in the service of politicized science that could have dire economic consequences for the nation and planet,” Rand Simberg wrote in\\xa0National Review\\xa0article in\\xa02012.\"\\n</statement>\\n\\nCategory number:<|end|>\\n<|assistant|>\\n 6', additional_kwargs={}, response_metadata={}, id='run-59010fb0-6251-4cf0-b135-086e4e5eb989-0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(responses), type(responses[0]), len(responses))\n",
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d332de25-51b1-4a45-a423-64f8f3f4b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_response(response : str):\n",
    "    parts = response.split(\"<|assistant|>\")\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13e5c8d5-7d3f-46cf-bb52-71a34669af39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 6_proponents_biased\n",
       "1                5_science_unreliable\n",
       "2                     1_not_happening\n",
       "3                5_science_unreliable\n",
       "4                     1_not_happening\n",
       "5               7_fossil_fuels_needed\n",
       "6                           3_not_bad\n",
       "7                 6_proponents_biased\n",
       "8                     1_not_happening\n",
       "9                         2_not_human\n",
       "10               5_science_unreliable\n",
       "11               5_science_unreliable\n",
       "12               5_science_unreliable\n",
       "13              7_fossil_fuels_needed\n",
       "14                          3_not_bad\n",
       "15                6_proponents_biased\n",
       "16                          3_not_bad\n",
       "17                6_proponents_biased\n",
       "18    4_solutions_harmful_unnecessary\n",
       "19               5_science_unreliable\n",
       "20              7_fossil_fuels_needed\n",
       "21                6_proponents_biased\n",
       "22               5_science_unreliable\n",
       "23                          3_not_bad\n",
       "24                              error\n",
       "25                6_proponents_biased\n",
       "26                     0_not_relevant\n",
       "27                              error\n",
       "28                              error\n",
       "29               5_science_unreliable\n",
       "30                              error\n",
       "31                              error\n",
       "32               5_science_unreliable\n",
       "33                              error\n",
       "34                6_proponents_biased\n",
       "35                              error\n",
       "36                              error\n",
       "37                              error\n",
       "38                              error\n",
       "39                              error\n",
       "40                              error\n",
       "41                              error\n",
       "42                              error\n",
       "43                              error\n",
       "44                              error\n",
       "45                              error\n",
       "46                              error\n",
       "47                              error\n",
       "48              7_fossil_fuels_needed\n",
       "49                6_proponents_biased\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = [r.content for r in responses]\n",
    "responses_s = pd.Series(prediction)\n",
    "assistant_responses = responses_s.apply(extract_assistant_response)\n",
    "y_pred = assistant_responses.apply(parse_output)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c5e0712-3680-4310-8ace-12662315af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluation(X_test, y_test, y_pred):\n",
    "    # Store results in a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "    results[\"correct\"] = results[\"y_test\"] == results[\"y_pred\"]\n",
    "\n",
    "    # Compute overall performance breakdown\n",
    "    correct = np.sum(results[\"correct\"])\n",
    "    unknown = np.sum(results[\"y_pred\"] == 'unknown')\n",
    "    errors = np.sum(results[\"y_pred\"] == 'error')\n",
    "    incorrect = len(results) - correct - errors\n",
    "    \n",
    "    performance = pd.DataFrame({\n",
    "        'Outcome': ['Correct', 'Incorrect', 'Unknown', 'Error'],\n",
    "        'Count': [correct,incorrect,unknown, errors]\n",
    "        })\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get category labels (sorted for consistency)\n",
    "    category_names = sorted(pd.Series(y_test).unique())\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    class_accuracy = results.groupby(\"y_test\")[\"correct\"].mean().reindex(category_names, fill_value=0).values\n",
    "    precision = precision_score(y_test, y_pred, average=None, labels=category_names, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=None, labels=category_names, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=None, labels=category_names, zero_division=0)\n",
    "\n",
    "    # Store per-category metrics\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"Category\": category_names,\n",
    "        \"Accuracy\": class_accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "\n",
    "    return results, accuracy, metrics_df, performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1927a12c-4ba9-43fa-b578-7764c0f312f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cd14c47-a0e4-4d33-a8ca-252aef25bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, accuracy, metrics_df, performance = evaluation(X_test.tolist(), y_test.tolist(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc666645-d00b-40d5-bfb0-cab57e0bb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors : 0 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"errors : {round(performance.iloc[2]['Count'] / sum(performance['Count']) * 100)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea36b21f-620c-425b-9013-2b128823171a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Correct</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Incorrect</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Error</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outcome  Count\n",
       "0    Correct     15\n",
       "1  Incorrect     16\n",
       "2    Unknown      0\n",
       "3      Error     19"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ebb731-8440-4c93-afc8-0fba4b8232b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397eeed9-1290-4c5e-8260-31baef5603e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cd2dc-a040-4698-830f-0d78362f1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frugalai.utils.hardware import print_mac_specs\n",
    "\n",
    "print_mac_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0d3b68-9daf-4c2d-9e5e-2ad61b5f7c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ddba8-599c-408c-bf51-94dd5ae894e8",
   "metadata": {},
   "source": [
    "### Fine-tuning LLM text-classification with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9c33ec-f865-4394-b8d3-18ca5e96e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frugalai.utils.efficiency_tracker import FunctionTracker\n",
    "\n",
    "tracker = FunctionTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ce1ed-b610-4456-b581-e398ea18c5af",
   "metadata": {},
   "source": [
    "##### **Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c19abc-6ce1-440f-a9cc-c6205e0fbc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c78963e-dde7-42d8-9152-ff65455130a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"0_not_relevant\": 0,\n",
    "    \"1_not_happening\": 1,\n",
    "    \"2_not_human\": 2,\n",
    "    \"3_not_bad\": 3,\n",
    "    \"4_solutions_harmful_unnecessary\": 4,\n",
    "    \"5_science_unreliable\": 5,\n",
    "    \"6_proponents_biased\": 6,\n",
    "    \"7_fossil_fuels_needed\": 7,\n",
    "}\n",
    "\n",
    "id2label = {int(v): k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a41a70e-ece6-4660-becc-61a79f500439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ FunctionTimer: load_frugalai_dataset\n",
      "| time            00:00:08.2587\n",
      "| emissions       0.000000 CO2eq\n",
      "| energy consumed 0.000005 kWh\n",
      "\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "{'train': (4872, 2), 'test': (1219, 2)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "@tracker.track\n",
    "def load_frugalai_dataset():\n",
    "    \n",
    "    ds = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\")\n",
    "    ds = ds.select_columns(['quote', 'label'])\n",
    "    ds = ds.map(lambda x: {\"label\": label2id[x[\"label\"]]}, batched=False)\n",
    "    return ds\n",
    "\n",
    "ds = load_frugalai_dataset()\n",
    "print(type(ds))\n",
    "print(ds.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4af937-f0fd-438b-a9e1-076c829784ea",
   "metadata": {},
   "source": [
    "##### **Sample a balanced subset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8017185-40a6-4389-ba45-1fbe1f529296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Label distribution in 'train' split (dataset.DatasetDict):\n",
      "Category                                Count     Percentage\n",
      "------------------------------------------------------------\n",
      "0                                       1311      26.91%\n",
      "1                                       587       12.05%\n",
      "2                                       565       11.6%\n",
      "3                                       289       5.93%\n",
      "4                                       614       12.6%\n",
      "5                                       641       13.16%\n",
      "6                                       643       13.2%\n",
      "7                                       222       4.56%\n",
      "------------------------------------------------------------\n",
      "Total                                   4872      \n",
      "\n",
      "üîπ Label distribution in 'test' split (dataset.DatasetDict):\n",
      "Category                                Count     Percentage\n",
      "------------------------------------------------------------\n",
      "0                                       307       25.18%\n",
      "1                                       154       12.63%\n",
      "2                                       137       11.24%\n",
      "3                                       97        7.96%\n",
      "4                                       160       13.13%\n",
      "5                                       160       13.13%\n",
      "6                                       139       11.4%\n",
      "7                                       65        5.33%\n",
      "------------------------------------------------------------\n",
      "Total                                   1219      \n"
     ]
    }
   ],
   "source": [
    "from frugalai.utils.analytics import print_distribution\n",
    "\n",
    "print_distribution(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09390876-66ee-4b9c-9f3e-b86d4503ff0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['quote', 'label'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['quote', 'label'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "})\n",
      "\n",
      "üîπ Label distribution in 'train' split (dataset.DatasetDict):\n",
      "Category                                Count     Percentage\n",
      "------------------------------------------------------------\n",
      "0                                       5         12.5%\n",
      "1                                       5         12.5%\n",
      "2                                       5         12.5%\n",
      "3                                       5         12.5%\n",
      "4                                       5         12.5%\n",
      "5                                       5         12.5%\n",
      "6                                       5         12.5%\n",
      "7                                       5         12.5%\n",
      "------------------------------------------------------------\n",
      "Total                                   40        \n",
      "\n",
      "üîπ Label distribution in 'test' split (dataset.DatasetDict):\n",
      "Category                                Count     Percentage\n",
      "------------------------------------------------------------\n",
      "0                                       5         12.5%\n",
      "1                                       5         12.5%\n",
      "2                                       5         12.5%\n",
      "3                                       5         12.5%\n",
      "4                                       5         12.5%\n",
      "5                                       5         12.5%\n",
      "6                                       5         12.5%\n",
      "7                                       5         12.5%\n",
      "------------------------------------------------------------\n",
      "Total                                   40        \n"
     ]
    }
   ],
   "source": [
    "from frugalai.utils.sampling import sample_balanced_subset\n",
    "# N = lowest category count * nb of categories\n",
    "subset_ds = sample_balanced_subset(ds, N=40, seed=42)\n",
    "\n",
    "print(subset_ds)\n",
    "print_distribution(subset_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6fe9c-20a2-4700-99b0-7bf8f2a3eade",
   "metadata": {},
   "source": [
    "##### **Load tokenizer & model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce0edf9-fa30-49ec-afbb-add2d5f91cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenVINO/Phi-3-mini-4k-instruct-int4-ov'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Model\n",
    "MODEL_NAMES = {\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"phi3\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"Qphi3\": \"OpenVINO/Phi-3-mini-4k-instruct-int4-ov\",\n",
    "    \"qwen\": \"Qwen/Qwen2.5-0.5B\"\n",
    "}\n",
    "selected_model = \"Qphi3\"\n",
    "model_name = MODEL_NAMES[selected_model]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e353a59-4fbd-4eec-9b7b-cbbc6a1f3a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'train': (40, 2), 'test': (40, 2)},\n",
       " {'train': ['quote', 'label'], 'test': ['quote', 'label']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_ds.shape, subset_ds.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75621470-8c36-41fe-bd0f-d191dbe52bcd",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcb824-96ca-47de-a749-a71b17b8e232",
   "metadata": {},
   "source": [
    "- essayer avec et sans padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a75a29-e304-4c07-ac24-7d75ceb95d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "377b18ea-e36c-45d0-9fd8-56dbeff623d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ FunctionTimer: load_tokenizer\n",
      "| time            00:00:06.3313\n",
      "| emissions       0.000000 CO2eq\n",
      "| energy consumed 0.000001 kWh\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2797688a44e4a4a91e52631633cb0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4a5368277f40b1aa7633ab64d9aa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 40\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "@tracker.track\n",
    "def load_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_tokens)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "def preprocess_function(element):\n",
    "    return tokenizer(element[\"quote\"], truncation=True, max_length=max_tokens) #padding=\"max_length\"\n",
    "\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "tokenized_ds = subset_ds.map(preprocess_function, batched=True, remove_columns=[\"quote\"])\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce343146-0b6e-4f5c-994f-46707e4e16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual max lenght in train set : 128 tokens\n",
      "Average sequence length: 64.8\n"
     ]
    }
   ],
   "source": [
    "train_lengths = [len(x) for x in tokenized_ds[\"train\"][\"input_ids\"]]\n",
    "print('Actual max lenght in train set :', max(train_lengths), 'tokens')\n",
    "print(f\"Average sequence length: {sum(train_lengths)/len(train_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b7955a-9a04-4cc1-a986-388645011e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8f71d-f9c3-4a7e-889f-b3c8a467edb5",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97a8871-e381-4168-a618-3521eadcacd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "                    MEMORY USAGE REPORT                     \n",
      "============================================================\n",
      "\n",
      "-------------------- CPU MEMORY --------------------\n",
      "Total System Memory:         24.00 GB\n",
      "Available System Memory:     10.19 GB\n",
      "Used System Memory:          12.24 GB (57.5%)\n",
      "Current Process Memory:      0.48 GB\n",
      "\n",
      "-------------------- MPS MEMORY --------------------\n",
      "Tensor Allocated Memory:     0.00 GB\n",
      "Overhead (PyTorch Internal): 0.00 GB\n",
      "Driver Allocated Memory:     0.00 GB\n",
      "Recommended Maximum Memory:  16.00 GB\n",
      "Available in Memory Pool:    16.00 GB\n",
      "\n",
      "-------------------- TENSOR COUNTS --------------------\n",
      "CPU Tensors:                 0\n",
      "MPS Tensors:                 0\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.villa.massone/Library/Caches/pypoetry/virtualenvs/frugalai-BGGDvkbh-py3.12/lib/python3.12/site-packages/torch/__init__.py:1113: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    }
   ],
   "source": [
    "from frugalai.utils.monitoring import print_memory_status_across_devices\n",
    "\n",
    "print_memory_status_across_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16b48f-05dd-4fbe-8521-f811f5f1a082",
   "metadata": {},
   "source": [
    "- try with mps\n",
    "- if not, cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73fdf40a-9fd6-4e76-a3f2-f085fd1930ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# For MPS compatibility, we need to be careful with quantization settings\n",
    "# MPS doesn't fully support all quantization formats, so we'll use bfloat16 for training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,        # Use double quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",             # Quantization type\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute datatype\n",
    ")\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbde8617-cb26-4386-9f47-7e7341a061da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2b3fd9c3554ae6be1e23010727bbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/940 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800e75fcebb24cda9a5a14f969658c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/OpenVINO/Phi-3-mini-4k-instruct-int4-ov:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.OpenVINO.Phi-3-mini-4k-instruct-int4-ov.4f812530a007e205a0c04f17c8c28fb5c8015a3c.configuration_phi3.Phi3Config'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DiffLlamaConfig, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTJConfig, HeliumConfig, IBertConfig, JambaConfig, JetMoeConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NemotronConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     18\u001b[0m         model_name,\n\u001b[1;32m     19\u001b[0m         label2id\u001b[38;5;241m=\u001b[39mlabel2id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# trust_remote_code=True,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# Load a quantized model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#.to(device) # for smaller models : moves the entire model to the specified device / all or nothing\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.device :\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision model.dtype :\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Code/Frugal_AI/frugalai/utils/efficiency_tracker.py:23\u001b[0m, in \u001b[0;36mFunctionTracker.track.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m emission_tracker \u001b[38;5;241m=\u001b[39m EmissionsTracker(log_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m emission_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m execution_emissions \u001b[38;5;241m=\u001b[39m emission_tracker\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m     26\u001b[0m execution_energy_conso \u001b[38;5;241m=\u001b[39m emission_tracker\u001b[38;5;241m.\u001b[39mfinal_emissions_data\u001b[38;5;241m.\u001b[39menergy_consumed\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Loading model with AutoModelForSequenceClassification adds an \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# randomly initialized classification head : score.weight\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# it will be trained along with the LoRa parameters during FT\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel2id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid2label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#torch_dtype=dtype,\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/frugalai-BGGDvkbh-py3.12/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:567\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules.OpenVINO.Phi-3-mini-4k-instruct-int4-ov.4f812530a007e205a0c04f17c8c28fb5c8015a3c.configuration_phi3.Phi3Config'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DiffLlamaConfig, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTJConfig, HeliumConfig, IBertConfig, JambaConfig, JetMoeConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MobileBertConfig, ModernBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NemotronConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, Phi3Config, PhimoeConfig, PLBartConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from frugalai.utils.hardware import get_device\n",
    "\n",
    "@tracker.track\n",
    "def load_model(model_name):\n",
    "\n",
    "    device = get_device() \n",
    "\n",
    "    if device.type == \"cuda\" or device.type == \"mps\":\n",
    "        dtype = torch.float16\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "\n",
    "    # Loading model with AutoModelForSequenceClassification adds an \n",
    "    # randomly initialized classification head : score.weight\n",
    "    # it will be trained along with the LoRa parameters during FT\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        label2id=label2id,\n",
    "        id2label=id2label,\n",
    "        num_labels=8,\n",
    "        #torch_dtype=dtype,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "        # trust_remote_code=True,\n",
    "        \n",
    "        # Load a quantized model\n",
    "        #load_in_8bit=True,  # Enable 8-bit quantization, allow auto device allocation in that case to help manage\n",
    "        \n",
    "        # Enable model sharding to optimize memory allocation\n",
    "        # device_map=auto, # for larger models : model sharding : auto distribution of model layers across available hardware, \n",
    "        # splits a large model across GPU and CPU. Handled by Accelerate library\n",
    "        \n",
    "    #.to(device) # for smaller models : moves the entire model to the specified device / all or nothing\n",
    "\n",
    "model = load_model(model_name)\n",
    "\n",
    "print('model.device :', model.device)\n",
    "print('precision model.dtype :', model.dtype)\n",
    "print('model.framework :',  model.framework)\n",
    "print('model.is_gradient_checkpointing :',  model.is_gradient_checkpointing)\n",
    "print('model.is_parallelizable :',  model.is_parallelizable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dcbbda-e839-442a-987c-afb31f6fe648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frugalai.utils.monitoring import print_memory_status_across_devices\n",
    "\n",
    "print_memory_status_across_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62be31-5c0c-4756-aeb3-354b0b3c3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80c174-ded1-42ce-b50c-682c441bbbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45533f42-df6b-4c6d-852f-2bbcbd5e7da8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b89fd9-6ed4-49d5-8b46-ea18154ce828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b666d65-e607-46b1-9ff3-de5ba934f8ae",
   "metadata": {},
   "source": [
    "**LoRa Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853e476-e11f-4041-8ce8-37094650e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ee2e6-0f73-4b0b-a660-4f05cfb8bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02e024-8b75-46e7-854a-8bb6fea7471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "modules = find_all_linear_names(model) \n",
    "    # target_modules=[\n",
    "    #    \"self_attn.qkv_proj\",\n",
    "    #    \"self_attn.o_proj\",\n",
    "    #    \"mlp.gate_up_proj\",\n",
    "    #    \"mlp.down_proj\"\n",
    "    #],\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=modules,\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(type(model))\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed4b85-8d2c-4c51-8c9f-a843efc3b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf540d5-5113-4485-9a66-fc7b64fb7291",
   "metadata": {},
   "source": [
    "**Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce528e-ea19-4f7d-9c3b-a26ac1913961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd608c-b4f6-403a-893b-80ed733e37c0",
   "metadata": {},
   "source": [
    "**Training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d58f37-d635-4127-851e-d7fbcc22ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def get_training_args(output_dir=\"./results\"):\n",
    "    return TrainingArguments(\n",
    "        max_steps=500,  # Adjust based on your dataset\n",
    "        warmup_steps=50,\n",
    "        eval_steps=50,\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,                         # Use mixed precision\n",
    "    optim=\"adamw_torch\",\n",
    "    label_names=['label'],\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",                  # Disable reporting to wandb, etc.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds['train'],\n",
    "    eval_dataset=tokenized_ds['test'],\n",
    "    compute_metrics=compute_metrics\n",
    "    packing=True,                      # Enable packing for efficiency\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4bef4-eaba-4acd-a1ba-21fe492b5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimated_time_per_step = 0.5\n",
    "#num_training_steps = (len(tokenized_ds['train']) // 4) * 5\n",
    "#total_training_time = estimated_time_per_step * num_training_steps \n",
    "\n",
    "#print(f\"Estimated training time: {total_training_time / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92a108-a427-4907-ad9e-9967047c36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frugalai.utils.monitoring import estimate_ft_memory_requirements\n",
    "\n",
    "memory_estimates = estimate_ft_memory_requirements(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    training_args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c727d9-5472-4c9a-8a43-72a3cbcb88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3aebfe-7792-4604-8952-8a596b87928c",
   "metadata": {},
   "source": [
    "**Do a small test run to check if it's ok**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7b511-f9bd-4777-8161-7352019afd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb942cf-b508-4c28-8754-7caa077c9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7327627-1769-4244-bd72-45b00e9267b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print validation accuracy\n",
    "print(f\"Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516c909-40c5-4626-9536-627c796278d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frugalai",
   "language": "python",
   "name": "frugalai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
